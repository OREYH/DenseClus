{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Clustering is a very hard problem because there is never truly 'right' answer when labels are unknown.\n",
    "\n",
    "To complicate matters further there is **[No Free Lunch](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)** for clustering algorithms and while one algorithm might fit a certain dataset well, there are no guarantees that it will work on a different data the exact same way. \n",
    "Likewise, clustering is *\"strongly dependent on contexts, aims and decisions of the researcher\"* which adds fire to the argument that there is no such thing as a *\"universally optimal method that will just produce natural clusters\"* ( see [What Are True Clusters? Henning 2015](https://arxiv.org/abs/1502.02555) ).\n",
    "\n",
    "Moreover, clustering techniques that generalize well, such as KMeans, assume that data is numerical and sphere-shaped. Having data of mixed types with high dimensionality also presents challenges for the downstream clustering task as classical methods such as Principal Component Analysis PCA for dimensionality reduction do not work when categorical values are included. This leads to a conundrum for the practitioner where specific featurization schemes must be formalized - such as including only numerical values or transforming all to categorical and then using Multiple Correspondence Analysis MCA instead.\n",
    "\n",
    "The approach outlined here seeks to solve both the difficulty in finding a default clustering algorithm and to circumvent the difficulties represented when data is in a mixed type form. Using a combination of the [Uniform Manifold Approximation and Projection UMAP](https://arxiv.org/abs/1802.03426) and [Hierarchical Density Based Clustering HDBSCAN](https://arxiv.org/abs/1705.07321) we will first take mix type data and then map it into a dense lower dimensional space. From this dense space, we will then hierarchically group into clusters based on the density of points. The final result will provide a simple end to end to solution that can be applied on wide variety of data to find meaningful clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "from denseclus import DenseClus\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sns.set(rc={\"figure.figsize\": (10, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)  # set the random seed as best we can\n",
    "\n",
    "data_url = (\n",
    "    \"https://raw.githubusercontent.com/awslabs/aws-customer-churn-pipeline/main/data/churn.txt\"\n",
    ")\n",
    "df = pd.read_csv(data_url).sample(n=2000, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you will use a synthetic churn dataset for an imaginary telecommunications company with the outcome Churn? flagged as as either True (churned) or False (did not churn). Features include customer details such as plan and usage information. The churn dataset is publicly available and mentioned in the book [Discovering Knowledge in Data by Daniel T. Larose](https://www.amazon.com/dp/0470908742/). It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Phone\", \"Area Code\"], axis=1, inplace=True)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data consists of both categorical and numeric features.\n",
    "Generally, speaking this is problematic for traditional dimension reduction and clustering methods such as [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) as they rely input features to be numeric and assume that the values are shaped spherical in nature.\n",
    "\n",
    "With DenseClus this is not an issue because we use create UMAP embeddings for both categorical and numerical, combining the embedding space to output them into the densest space possible. Next HDBSCAN is run to group densities into clusters, resulting a groups of mixed-type data. \n",
    "\n",
    "All of this is done under the hood and just requires a `fit` call like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DenseClus(\n",
    "    random_state=SEED,\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    umap_combine_method=\"intersection_union_mapper\",\n",
    ")\n",
    "\n",
    "clf.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap the steps that happened are:\n",
    "\n",
    "1). Numerical features were taken out and then reduced into a *dense* UMAP embedding\n",
    "\n",
    "2) Categorical features got extracted and learned into a *dense* separate UMAP embedding\n",
    "\n",
    "3) The two embeddings were then combined with an intersection over union operation\n",
    "\n",
    "4) HDBSCAN uses density-based spatial clustering to hierarchical-fashion to extract clusters from the combined space\n",
    "\n",
    "All of these features are now attached as usable `DenseClus` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Embedding Results\n",
    "\n",
    "Verify the embeddings are now densely shaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(clf.n_components):\n",
    "    sns.kdeplot(clf.mapper_.embedding_[:, i], shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.jointplot(x=clf.mapper_.embedding_[:, 0], y=clf.mapper_.embedding_[:, -1], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection of Cluster Results\n",
    "\n",
    "The clustering results are extricable from the object.\n",
    "\n",
    "X groups formed into clusters, with the largest constituting Y% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clf.score()\n",
    "\n",
    "print(labels, \"\\n\")\n",
    "print(pd.DataFrame(labels).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with DBSCAN, labels of -1 are flagged as noise and all lables have an associated noise probability score.\n",
    "\n",
    "In practice, these can be thrown out or put into an 'other' cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.jointplot(\n",
    "    x=clf.mapper_.embedding_[:, 0],\n",
    "    y=clf.mapper_.embedding_[:, 1],\n",
    "    hue=labels,\n",
    "    kind=\"kde\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the clusters formed based on the densities represented in the reduced space.\n",
    "\n",
    "Specifically, 4 groups got identified by the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since HDBSCAN is hierarchical in nature and splits based on a tree.\n",
    "\n",
    "Instead of using the default method, you used `leaf` which splits into smaller groups along the tree. \n",
    "\n",
    "The below plot shows that the default setting would find two clusters total but since we split along the leaf there are four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = clf.hdbscan_.condensed_tree_.plot(\n",
    "    select_clusters=True,\n",
    "    selection_palette=sns.color_palette(\"deep\", np.unique(labels).shape[0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling the Clusters\n",
    "\n",
    "Finally, once clusters are formed, it's common practice to then describe what each one means.\n",
    "\n",
    "Here, descriptive statistics is actually a very powerful (and efficient) tool to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"segment\"] = clf.score()\n",
    "\n",
    "numerics = df.select_dtypes(include=[int, float]).drop([\"segment\"], 1).columns.tolist()\n",
    "\n",
    "df[numerics + [\"segment\"]].groupby([\"segment\"]).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore group `-1`.\n",
    "\n",
    "You can see that only first segment `0` has the shortest day minutes and high night calls. These are perhaps customers that prefer calling (and getting called) in the night.\n",
    "\n",
    "The second segment `1` has the highest day minutes and day charges. These are customer that prefer calling (and getting called) during the day.\n",
    "\n",
    "This type of profiling is possible with the other segments as well, coming up with a description of what attributes they constitute and how they relate to your objective.\n",
    "\n",
    "Again, this is just profiling but descriptive statistics are revealing of what patterns are captured.\n",
    "\n",
    "A similar type of analysis is possible with categorical features, with a reference shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.select_dtypes(include=[\"object\"]).drop([\"State\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categorical.columns:\n",
    "    df.groupby([\"segment\"] + [c]).size().plot(\n",
    "        kind=\"bar\", color=sns.color_palette(\"deep\", np.unique(labels).shape[0])\n",
    "    )\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
