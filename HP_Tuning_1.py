#!/usr/bin/env python3
"""tune_denseclus.py
-------------------------------------------------------
UMAP + HDBSCAN í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ìŠ¤í¬ë¦½íŠ¸ (ì¬í˜„ì„± ê°•í™” ë²„ì „)
 - YAML ì €ì¥
 - ê²°ê³¼ ë¡œê·¸ëŠ” results í´ë”ì—, YAML íŒŒì¼ì€ yaml í´ë”ì— ì €ì¥
 - `set_global_seed()` ë¡œ NumPyÂ·randomÂ·PYTHONHASHSEED ëª¨ë‘ ê³ ì • â†’ ë°˜ë³µ ì‹¤í–‰ ì‹œ
   ë™ì¼í•œ coverage / DBCV ë³´ì¥
-------------------------------------------------------
ì‚¬ìš© ì˜ˆ)
    python tune_denseclus.py \
        --data_path ./data/train.csv \
        --save_name best_denseclus \
        --sample 20000 \
        --seed 42
    # ê²°ê³¼ íŒŒì¼ì€ yaml/best_denseclus_<ë‚ ì§œì‹œê°„>.yaml, 
    # ë¡œê·¸ íŒŒì¼ì€ results/best_denseclus_<ë‚ ì§œì‹œê°„>.log ë¡œ ì €ì¥
"""

import psutil, threading
import os, time
import argparse
import warnings
import random
from itertools import product
import logging
from datetime import datetime

import numpy as np
import pandas as pd
import yaml
from tqdm import tqdm

from denseclus import DenseClus

def monitor_cpu():
    while True:
        print(f"CPU usage: {psutil.cpu_percent()}%")
        time.sleep(10)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê²½ê³  ì–µì œ (sklearn force_all_finite)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.filterwarnings(
    "ignore", category=FutureWarning, message=".*force_all_finite.*"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œë“œ ê³ ì • ìœ í‹¸ë¦¬í‹° (ì¬í˜„ì„± í™•ë³´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def set_global_seed(seed: int):
    """random / numpy / hash seed ê³ ì •"""
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¸ì íŒŒì„œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_args():
    p = argparse.ArgumentParser(description="DenseClus í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    p.add_argument("--data_path", default='./data/flat-training.csv', help="CSV ë°ì´í„° ê²½ë¡œ")
    p.add_argument("--save_path", default='./results', help="ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    p.add_argument("--method", type=str, 
                   choices=["intersection", "union", "contrast", "intersection_union_mapper", "ensemble"],
                   default='intersection_union_mapper')
    p.add_argument("--sample", type=int, default=None, help="íŠœë‹ìš© ìƒ˜í”Œ ìˆ˜")
    p.add_argument("--dropna", action="store_true", help="ê²°ì¸¡ ì»¬ëŸ¼ ì œê±° ì—¬ë¶€")
    p.add_argument("--seed", type=int, default=42, help="ëœë¤ ì‹œë“œ")
    p.add_argument("--max_clusters", type=int, default=10, help='ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜')
    return p.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¡œë”© & ìƒ˜í”Œë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def read_data(path: str, sample: int, dropna: bool, seed: int):
    set_global_seed(seed)  # ìƒ˜í”Œë§ê¹Œì§€ ì‹œë“œ ê³ ì •
    df = pd.read_csv(path)
    if dropna:
        df = df.dropna(axis=1)
    if sample is not None and sample < len(df):
        df = df.sample(sample, random_state=seed)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í‰ê°€ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cluster_uniformity(counts: pd.DataFrame, eps: float = 1e-12) -> float:
    """
    0(ìµœì•…) ~ 1(ìµœê³ ) ì‚¬ì´ ê· í˜• ì ìˆ˜ ë°˜í™˜.
    """
    counts_arr = counts.loc[counts['cluster'] != -1, "count"].values # -1 ì œì™¸
    
    k = len(counts_arr) # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    p = counts_arr / counts_arr.sum() # ë¹„ìœ¨
    entropy = -np.sum(p * np.log(p + eps))

    return float(entropy / np.log(k)) # ì •ê·œí™”


def evaluate(method: str, umap_params: dict, hdbscan_params: dict, data: pd.DataFrame, seed: int, logger: logging.Logger):
    set_global_seed(seed)
    clf = DenseClus(
        random_state=seed,
        umap_combine_method=method,
        umap_params=umap_params,
        hdbscan_params=hdbscan_params,
    )
    clf.fit(data)
    labels = clf.labels_

    try:
        cnts = pd.DataFrame(clf.evaluate())[0].value_counts()
        cnts = cnts.reset_index()
        cnts.columns = ['cluster', 'count']
        cnts = cnts.sort_values(['cluster'], ignore_index=True)

        uniformity = cluster_uniformity(cnts)

        coverage = (labels >= 0).mean()  # êµ°ì§‘ì— ì†í•œ ë¹„ìœ¨
        dbcv = clf.hdbscan_.relative_validity_ # type: ignore
        n_cluster = len(np.unique(labels[labels >= 0]))

        return coverage, dbcv, n_cluster, uniformity
    
    except Exception as e:
        # ë¡œê·¸
        err_message = f"[WARN] evalute ì‹¤íŒ¨: {e}"
        logger.info(err_message)
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":

    # t = threading.Thread(target=monitor_cpu, daemon=True)
    # t.start()

    args = get_args()
    set_global_seed(args.seed)
    
    # ì €ì¥í•  ê¸°ë³¸ ì„¸íŒ…ê°’
    setting = {
        "seed": args.seed,
        "n_samples": args.sample,
        "dropna": args.dropna,
        "method": args.method,
        "max_clusters": args.max_clusters
    }


    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    save_path = os.path.join(args.save_path, timestamp)
    os.makedirs(save_path)

    # íŒŒì¼ ì €ì¥ ê²½ë¡œ
    log_path = os.path.join(save_path, "train.log")
    yaml_path = os.path.join(save_path, "config.yaml")
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(message)s",
        handlers=[
            logging.FileHandler(log_path, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )
    logger = logging.getLogger(__name__)

    # ë°ì´í„° ì¤€ë¹„
    df = read_data(args.data_path, args.sample, args.dropna, args.seed)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
    umap_grid = [
        {
            "categorical": {"n_neighbors": n_cat, "min_dist": m_cat},
            "numerical":   {"n_neighbors": n_num, "min_dist": m_num},
            "combined":    {"n_neighbors": n_com, "min_dist": m_com},
        }
        for n_cat, m_cat, n_num, m_num, n_com, m_com in product(
            [15, 30],
            [0.0, 0.2],
            [20, 40],
            [0.0, 0.2],
            [5, 10],
            [0.0, 0.2],
        )
    ]

    hdbscan_grid = [
        {"min_samples": ms, "min_cluster_size": mcs, "gen_min_span_tree": True}
        for ms, mcs in product([50], [500, 1000, 2000])
    ]

    total_iter = len(umap_grid) * len(hdbscan_grid) // 2
    pbar = tqdm(total=total_iter, desc="Grid Search", ncols=110, colour="cyan")

    best_score = float("-inf")
    best_params = None

    for u_params in umap_grid:
        for h_params in hdbscan_grid:
            result = evaluate(args.method, u_params, h_params, df, args.seed, logger)

            if result is None:
                continue
            coverage, dbcv, n_clusters, uniformity = result

            msg = f"n_clusters: {n_clusters}"
            logger.info(msg)
            pbar.write("\n " + msg)

            # í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ args.max_clustersì„ ë„˜ì–´ê°€ë©´ ë‹¤ìŒ ë£¨í”„ë¡œ ì§„í–‰
            if n_clusters > args.max_clusters:
                logger.info("í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ 10ì„ ì´ˆê³¼í•˜ë¯€ë¡œ ë‹¤ìŒ ë£¨í”„ë¡œ ë„˜ê¹ë‹ˆë‹¤.")
                logger.info(f"score={best_score:.4f} | cov={coverage:.4f}, dbcv={dbcv:.4f} | uniformity={uniformity:.4f}")
                continue

            # uniformityê°€ 0.5 ë¯¸ë§Œì´ë©´ ë‹¤ìŒ ë£¨í”„ë¡œ ì§„í–‰
            if uniformity < 0.5:
                logger.info("uniformityê°€ 0.5ë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ë‹¤ìŒ ë£¨í”„ë¡œ ë„˜ê¹ë‹ˆë‹¤.")
                logger.info(f"score={best_score:.4f} | cov={coverage:.4f}, dbcv={dbcv:.4f} | uniformity={uniformity:.4f}")
                continue

            score = coverage * dbcv * uniformity

            if score > best_score:
                best_score = score

                best_params = {
                    "setting": setting,
                    "n_clusters": n_clusters,
                    "umap_params": u_params,
                    "hdbscan_params": h_params }

                with open(yaml_path, "w", encoding="utf-8") as f:
                    yaml.dump(best_params, f, sort_keys=False, allow_unicode=True, indent=4)

                best_msg = (
                    f"ğŸ“ˆ New best â†’ score={best_score:.4f} | cov={coverage:.4f}, dbcv={dbcv:.4f} | uniformity={uniformity:.4f}" )
                logger.info(best_msg)
                pbar.write("\n")

            pbar.update(1)

    pbar.close()

    logger.info("âœ… íŠœë‹ ì™„ë£Œ!")
    logger.info(f"YAML saved to: {yaml_path}")
    logger.info(f"Log saved to : {log_path}")
    logger.info(f"Best score  : {best_score}")
    logger.info("Best params :\n" + yaml.safe_dump(best_params, sort_keys=False))
